{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy Neural Network Demonstration\n",
    "\n",
    "This notebook demonstrates how to use the `numpy-neural-network` library, which we have built from scratch.\n",
    "\n",
    "We will cover:\n",
    "1.  **Mathematical Foundations**: A brief overview of the math behind our layers.\n",
    "2.  **Running Scripts**: How to use the pre-built training scripts.\n",
    "3.  **Manual Experiment**: How to import library components to build, train, and evaluate a custom model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mathematical Foundations\n",
    "\n",
    "Our network is built on the principle of backpropagation, which is a clever application of the chain rule from calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation (The Chain Rule)\n",
    "\n",
    "Imagine a simple network: `Loss = L(y_pred, y_true)` where `y_pred = f(z)` and `z = g(x, W)`.\n",
    "\n",
    "To update our weights `W`, we need to find how the `Loss` changes with respect to `W` (i.e., $\\frac{\\partial L}{\\partial W}$). We use the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y_{pred}} \\cdot \\frac{\\partial y_{pred}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}$$\n",
    "\n",
    "In our code:\n",
    "1.  `loss_fn.backward()` computes $\\frac{\\partial L}{\\partial y_{pred}}$ (the *upstream gradient*).\n",
    "2.  This gradient is passed to the `backward()` method of layer `f`, which multiplies it by its local gradient ($\\frac{\\partial y_{pred}}{\\partial z}$) and passes the result ($\\frac{\\partial L}{\\partial z}$) *downstream*.\n",
    "3.  This new gradient is received by layer `g`, which computes $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Layers and Activations\n",
    "\n",
    "**Linear Layer:**\n",
    "-   **Forward:** $y = xW + b$\n",
    "-   **Backward:** Computes $\\frac{\\partial L}{\\partial W} = x^T \\cdot \\frac{\\partial L}{\\partial y}$, $\\frac{\\partial L}{\\partial b} = \\sum \\frac{\\partial L}{\\partial y}$, and $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot W^T$.\n",
    "\n",
    "**ReLU Activation:**\n",
    "-   **Forward:** $f(x) = \\max(0, x)$\n",
    "-   **Backward:** The local gradient is 1 for $x > 0$ and 0 otherwise. It acts as a \"gate,\" only allowing positive gradients to pass through. This sparsifies the network and helps prevent the vanishing gradient problem.\n",
    "\n",
    "**Batch Normalization:**\n",
    "-   **Forward:** Normalizes activations within a batch: $\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}$. Then scales and shifts: $y = \\gamma \\hat{x} + \\beta$.\n",
    "-   **Backward:** This is the most complex backward pass, as the gradient must flow back through $\\gamma$, $\\beta$, and the normalization statistics ($\\mu_B$ and $\\sigma^2_B$) to the input $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "**MSE (Mean Squared Error) Loss:**\n",
    "-   **Forward:** $L = \\frac{1}{N} \\sum (y_{pred} - y_{true})^2$\n",
    "-   **Backward:** Used for regression. The gradient is $\\frac{\\partial L}{\\partial y_{pred}} = \\frac{2}{N} (y_{pred} - y_{true})$.\n",
    "\n",
    "**Softmax Cross-Entropy Loss:**\n",
    "-   **Forward:** We combine two functions for numerical stability:\n",
    "    1.  **Softmax:** $P_i = \\frac{e^{z_i}}{\\sum e^{z_j}}$ (converts raw scores, or *logits* $z$, to probabilities $P$).\n",
    "    2.  **Cross-Entropy:** $L = -\\frac{1}{N} \\sum y_i \\log(P_i)$ (penalizes low probabilities for the true class $y_i$).\n",
    "-   **Backward:** When combined, the gradient is the very simple and stable: $\\frac{\\partial L}{\\partial z} = \\frac{1}{N} (P - Y_{onehot})$. This is the initial *upstream gradient*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running Pre-built Training Scripts\n",
    "\n",
    "First, we need to add our `src` directory to the Python path so we can import our library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the path\n",
    "# Assumes notebook.ipynb is in the root, and src is in the root\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "scripts_path = os.path.abspath(os.path.join(os.getcwd(), 'scripts'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "# Setup the logger (optional but good practice)\n",
    "from utils.logger import setup_logger\n",
    "setup_logger(log_dir=\"logs\", log_file=\"notebook.log\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the training scripts directly from the command line, or by using `!` in the notebook. These scripts handle all the argument parsing, model building, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the MNIST training script with custom hyperparameters for 5 epochs\n",
    "!python scripts/train_mnist.py --epochs 5 --lr 0.01 --batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the regression script for 10 epochs\n",
    "!python scripts/train_regression.py --epochs 10 --lr 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Manually Building and Training a Model\n",
    "\n",
    "This section shows how to use the library components to build a custom experiment from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import Sequential\n",
    "from layers import Linear, ReLU, BatchNorm\n",
    "from losses import SoftmaxCrossEntropyLoss\n",
    "from solver import Solver\n",
    "from utils.data_utils import load_fashion_mnist\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "# Let's do a simple binary classification problem:\n",
    "# Class 0: T-shirt/top\n",
    "# Class 1: Trouser\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_fashion_mnist(filter_classes=[0, 1])\n",
    "data = {\n",
    "    'X_train': X_train, 'y_train': y_train,\n",
    "    'X_val': X_val, 'y_val': y_val\n",
    "}\n",
    "\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "print(f\"Num classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Model\n",
    "# A simple model for a simple 2-class problem\n",
    "input_dim = X_train.shape[1] # 784\n",
    "output_dim = len(np.unique(y_train)) # 2\n",
    "\n",
    "model = Sequential(\n",
    "    layers=[\n",
    "        Linear(in_dim=input_dim, out_dim=50),\n",
    "        BatchNorm(dim=50),\n",
    "        ReLU(),\n",
    "        Linear(in_dim=50, out_dim=output_dim)\n",
    "    ],\n",
    "    loss_fn=SoftmaxCrossEntropyLoss(),\n",
    "    reg=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configure Solver\n",
    "solver = Solver(\n",
    "    model, data,\n",
    "    task_type='classification',\n",
    "    update_rule='sgd_momentum',\n",
    "    optim_config={'learning_rate': 1e-3, 'momentum': 0.9},\n",
    "    lr_decay=0.95,\n",
    "    num_epochs=5,\n",
    "    batch_size=32,\n",
    "    print_every=200, # Print every 200 iterations\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train!\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot Results\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Training Loss')\n",
    "plt.plot(solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.plot(solver.val_metric_history, label='Validation')\n",
    "plt.plot(solver.train_metric_history, label='Train (sub-sampled)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Check Final Test Accuracy\n",
    "test_acc = solver.check_metric(X_test, y_test)\n",
    "print(f\"Final Test Accuracy on {len(np.unique(y_test))} classes: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
